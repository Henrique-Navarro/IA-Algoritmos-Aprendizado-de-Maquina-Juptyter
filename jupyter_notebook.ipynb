{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "735901ea",
   "metadata": {},
   "source": [
    "> Trabalho da disciplina de Inteligência Artificial sobre Algoritmos de Aprendizado de Máquina.\n",
    ">> Feito por: **Henrique Navarro Morais.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1ea963",
   "metadata": {},
   "source": [
    "# Importações necessárias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ab30639",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d91168c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instação da extensão 'Collapsible Headings' para melhor visualização dos cabeçalhos\n",
    "import os\n",
    "os.system('pip install -q jupyter_contrib_nbextensions')\n",
    "os.system('jupyter contrib nbextension install --user')\n",
    "os.system('jupyter nbextension enable collapsible_headings/main --quiet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3e3047",
   "metadata": {},
   "source": [
    "# Sobre a Base de Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6487221b",
   "metadata": {},
   "source": [
    "> A base de dados possui diversos atributos sobre diversas pessoas adultas dos EUS, e seu principal objetivo é apontar se a renda do indivíduo é inferior ou superior a 50K.\n",
    ">\n",
    "> Link da base de dados: https://archive.ics.uci.edu/ml/datasets/Adult"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f680d8f7",
   "metadata": {},
   "source": [
    " > Foram feitas diversas operações na base de dados original, de forma que os dados foram tratados, limpos, convertidos e padronizados. Justamante para melhorar a qualidade e a confiabilidade dos dados a fim de serem usados por algoritmos de aprendizado de máquina.\n",
    "> \n",
    "> Será feita a importação de 2 versões da base de dados, uma versão sem os atributos desnecessários, sem os exemplos denecessários e duplicados, e outra com essas mesmas características, porém, com todos os dados convertidos para numéricos e padronizados.\n",
    ">\n",
    "> O atributo de saída da base de dados é a **Renda**, de forma que na base de dados numérica, padronizada com valores de -10 a +10, os exemplos com valor igual a <=50K serão representadas como -10, já os exemplos que assumirem o valor de >50K, serão representados como +10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f0ce9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_clean = pd.read_csv('./databases/db_clean.csv')\n",
    "db = pd.read_csv('./databases/db_num_ree.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe735b47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Idade</th>\n",
       "      <th>Classe Trabalhadora</th>\n",
       "      <th>Educacao</th>\n",
       "      <th>Num Educacao</th>\n",
       "      <th>Ocupacao</th>\n",
       "      <th>Ganho de Capital</th>\n",
       "      <th>Perda de Capital</th>\n",
       "      <th>Horas por Semana</th>\n",
       "      <th>Renda</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Idade Classe Trabalhadora    Educacao  Num Educacao            Ocupacao  \\\n",
       "0     39           State-gov   Bachelors            13        Adm-clerical   \n",
       "1     50    Self-emp-not-inc   Bachelors            13     Exec-managerial   \n",
       "2     38             Private     HS-grad             9   Handlers-cleaners   \n",
       "3     53             Private        11th             7   Handlers-cleaners   \n",
       "4     28             Private   Bachelors            13      Prof-specialty   \n",
       "\n",
       "   Ganho de Capital  Perda de Capital  Horas por Semana   Renda  \n",
       "0              2174                 0                40   <=50K  \n",
       "1                 0                 0                13   <=50K  \n",
       "2                 0                 0                40   <=50K  \n",
       "3                 0                 0                40   <=50K  \n",
       "4                 0                 0                40   <=50K  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Idade</th>\n",
       "      <th>Classe Trabalhadora</th>\n",
       "      <th>Educacao</th>\n",
       "      <th>Num Educacao</th>\n",
       "      <th>Ocupacao</th>\n",
       "      <th>Ganho de Capital</th>\n",
       "      <th>Perda de Capital</th>\n",
       "      <th>Horas por Semana</th>\n",
       "      <th>Renda</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-3.230769</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>-10.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>-10.000000</td>\n",
       "      <td>-8.277065</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>-0.370370</td>\n",
       "      <td>-10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.153846</td>\n",
       "      <td>-7.5</td>\n",
       "      <td>-10.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>-8.571429</td>\n",
       "      <td>-10.000000</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>-7.037037</td>\n",
       "      <td>-10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-3.538462</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>-8.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>-7.142857</td>\n",
       "      <td>-10.000000</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>-0.370370</td>\n",
       "      <td>-10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.076923</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>-7.333333</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>-7.142857</td>\n",
       "      <td>-10.000000</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>-0.370370</td>\n",
       "      <td>-10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-6.615385</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>-10.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>-5.714286</td>\n",
       "      <td>-10.000000</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>-0.370370</td>\n",
       "      <td>-10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Idade  Classe Trabalhadora   Educacao  Num Educacao   Ocupacao  \\\n",
       "0 -3.230769                -10.0 -10.000000      6.000000 -10.000000   \n",
       "1  0.153846                 -7.5 -10.000000      6.000000  -8.571429   \n",
       "2 -3.538462                 -5.0  -8.666667      0.666667  -7.142857   \n",
       "3  1.076923                 -5.0  -7.333333     -2.000000  -7.142857   \n",
       "4 -6.615385                 -5.0 -10.000000      6.000000  -5.714286   \n",
       "\n",
       "   Ganho de Capital  Perda de Capital  Horas por Semana  Renda  \n",
       "0         -8.277065             -10.0         -0.370370  -10.0  \n",
       "1        -10.000000             -10.0         -7.037037  -10.0  \n",
       "2        -10.000000             -10.0         -0.370370  -10.0  \n",
       "3        -10.000000             -10.0         -0.370370  -10.0  \n",
       "4        -10.000000             -10.0         -0.370370  -10.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(db_clean.head())\n",
    "display(db.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18908cd",
   "metadata": {},
   "source": [
    "# Técnica de Validação"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f5f89c",
   "metadata": {},
   "source": [
    "> O método hold-out envolve a divisão do conjunto de dados em dois subconjuntos distintos: um conjunto de **treinamento** e um conjunto de **teste.**\n",
    "> \n",
    "> A base de dados analisada foi dividida **20%** dos exemplos serão usados como **testes** e os **80%** restantes dos exemplos serão usados para **treino.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7857f4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(dataset, test_ratio):\n",
    "    dataset_array = dataset.to_numpy()\n",
    "    num_samples = len(dataset_array)\n",
    "    np.random.shuffle(dataset_array)\n",
    "    num_test_samples = int(num_samples * test_ratio)\n",
    "    train_data = dataset_array[num_test_samples:]\n",
    "    test_data = dataset_array[:num_test_samples]\n",
    "    train_data = pd.DataFrame(train_data, columns=dataset.columns)\n",
    "    test_data = pd.DataFrame(test_data, columns=dataset.columns)\n",
    "    \n",
    "    return train_data, test_data\n",
    "\n",
    "db_treino, db_teste = split_dataset(db, test_ratio=0.2)\n",
    "#db_clean_treino, db_clean_teste = split_dataset(db_clean, test_ratio=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3bc98b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho subconjunto de treino: 17920 exemplos\n",
      "Tamanho subconjunto de teste: 4480 exemplos\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Idade</th>\n",
       "      <th>Classe Trabalhadora</th>\n",
       "      <th>Educacao</th>\n",
       "      <th>Num Educacao</th>\n",
       "      <th>Ocupacao</th>\n",
       "      <th>Ganho de Capital</th>\n",
       "      <th>Perda de Capital</th>\n",
       "      <th>Horas por Semana</th>\n",
       "      <th>Renda</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.923077</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>-3.333333</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-8.571429</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>-1.515152</td>\n",
       "      <td>0.864198</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.076923</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>-8.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>-4.285714</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>-10.000000</td>\n",
       "      <td>-0.370370</td>\n",
       "      <td>-10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-6.615385</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>-8.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>-2.011019</td>\n",
       "      <td>-0.370370</td>\n",
       "      <td>-10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Idade  Classe Trabalhadora  Educacao  Num Educacao  Ocupacao  \\\n",
       "0  2.923077                 -5.0 -3.333333      2.000000 -8.571429   \n",
       "1  1.076923                 -5.0 -8.666667      0.666667 -4.285714   \n",
       "2 -6.615385                 -5.0 -8.666667      0.666667  0.000000   \n",
       "\n",
       "   Ganho de Capital  Perda de Capital  Horas por Semana  Renda  \n",
       "0             -10.0         -1.515152          0.864198   10.0  \n",
       "1             -10.0        -10.000000         -0.370370  -10.0  \n",
       "2             -10.0         -2.011019         -0.370370  -10.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Idade</th>\n",
       "      <th>Classe Trabalhadora</th>\n",
       "      <th>Educacao</th>\n",
       "      <th>Num Educacao</th>\n",
       "      <th>Ocupacao</th>\n",
       "      <th>Ganho de Capital</th>\n",
       "      <th>Perda de Capital</th>\n",
       "      <th>Horas por Semana</th>\n",
       "      <th>Renda</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-6.923077</td>\n",
       "      <td>-2.5</td>\n",
       "      <td>-10.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>7.142857</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>-1.604938</td>\n",
       "      <td>-10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.692308</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>-7.333333</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>2.857143</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>-2.345679</td>\n",
       "      <td>-10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-6.000000</td>\n",
       "      <td>-7.5</td>\n",
       "      <td>-0.666667</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>-1.428571</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>-0.370370</td>\n",
       "      <td>-10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Idade  Classe Trabalhadora   Educacao  Num Educacao  Ocupacao  \\\n",
       "0 -6.923077                 -2.5 -10.000000      6.000000  7.142857   \n",
       "1  1.692308                 -5.0  -7.333333     -2.000000  2.857143   \n",
       "2 -6.000000                 -7.5  -0.666667      3.333333 -1.428571   \n",
       "\n",
       "   Ganho de Capital  Perda de Capital  Horas por Semana  Renda  \n",
       "0             -10.0             -10.0         -1.604938  -10.0  \n",
       "1             -10.0             -10.0         -2.345679  -10.0  \n",
       "2             -10.0             -10.0         -0.370370  -10.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f'Tamanho subconjunto de treino: {db_treino.shape[0]} exemplos\\nTamanho subconjunto de teste: {db_teste.shape[0]} exemplos')\n",
    "display(db_treino.head(3))\n",
    "display(db_teste.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db19656",
   "metadata": {},
   "source": [
    "# Métricas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a152931",
   "metadata": {},
   "source": [
    "> Converter dados não numéricos em uma representação numérica é uma etapa crucial para garantir que os algoritmos possam processar e extrair insights úteis dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b004e2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_labels_to_numeric(labels):\n",
    "    unique_labels = list(set(labels))\n",
    "    label_map = {label: i for i, label in enumerate(unique_labels)}\n",
    "    numeric_labels = [label_map[label] for label in labels]\n",
    "    return numeric_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce079076",
   "metadata": {},
   "source": [
    "> **Acurácia:** é uma medida que indica a taxa de classificações corretas em relação ao total de amostras. É uma medida geral de desempenho do modelo, indicando a proporção de previsões corretas em relação ao número total de previsões."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82ee71ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(y_true, y_pred):\n",
    "    correct_predictions = 0\n",
    "    total_predictions = len(y_true)\n",
    "\n",
    "    for true_label, predicted_label in zip(y_true, y_pred):\n",
    "        if true_label == predicted_label:\n",
    "            correct_predictions += 1\n",
    "\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d8f4e6",
   "metadata": {},
   "source": [
    "> **Precisão:** é uma medida que indica a proporção de verdadeiros positivos em relação ao total de positivos previstos (verdadeiros positivos + falsos positivos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03d1faf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_precision(y_true, y_pred):\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "\n",
    "    for true_label, predicted_label in zip(y_true, y_pred):\n",
    "        if predicted_label == 1:\n",
    "            if true_label == predicted_label:\n",
    "                true_positives += 1\n",
    "            else:\n",
    "                false_positives += 1\n",
    "\n",
    "    if true_positives + false_positives == 0:\n",
    "        precision = 0\n",
    "    else:\n",
    "        precision = true_positives / (true_positives + false_positives)\n",
    "\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2ac7c4",
   "metadata": {},
   "source": [
    "> **Recall:** também conhecido como taxa de verdadeiros positivos ou sensibilidade, é uma medida que indica a proporção de verdadeiros positivos em relação ao total de positivos reais (verdadeiros positivos + falsos negativos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46845ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_recall(y_true, y_pred):\n",
    "    true_positives = 0\n",
    "    false_negatives = 0\n",
    "\n",
    "    for true_label, predicted_label in zip(y_true, y_pred):\n",
    "        if true_label == predicted_label == 1:\n",
    "            true_positives += 1\n",
    "        elif true_label == 1 and predicted_label == 0:\n",
    "            false_negatives += 1\n",
    "\n",
    "    if true_positives + false_negatives == 0:\n",
    "        recall = 0\n",
    "    else:\n",
    "        recall = true_positives / (true_positives + false_negatives)\n",
    "\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81223b1",
   "metadata": {},
   "source": [
    "> **Matriz de confusão:** uma tabela que mostra o desempenho do modelo de classificação em relação às classes reais. Ela apresenta a contagem de verdadeiros positivos, falsos positivos, verdadeiros negativos e falsos negativos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0b649f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_confusion_matrix(y_true, y_pred):\n",
    "    true_positives = 0\n",
    "    true_negatives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "\n",
    "    for true_label, predicted_label in zip(y_true, y_pred):\n",
    "        if true_label == predicted_label == 1:\n",
    "            true_positives += 1\n",
    "        elif true_label == predicted_label == 0:\n",
    "            true_negatives += 1\n",
    "        elif true_label == 0 and predicted_label == 1:\n",
    "            false_positives += 1\n",
    "        elif true_label == 1 and predicted_label == 0:\n",
    "            false_negatives += 1\n",
    "\n",
    "    confusion = [[true_negatives, false_positives], [false_negatives, true_positives]]\n",
    "\n",
    "    return confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b447f5",
   "metadata": {},
   "source": [
    "# Algoritmo Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d295bf9b",
   "metadata": {},
   "source": [
    "> **Algoritmo baseline** é um método simples que serve como ponto de referência para avaliar o desempenho de modelos mais avançados. Nesse caso, foi utilizado um algoritmo que calcula a frequência de cada classe no conjunto de treinamento e, em seguida, atribui a classe majoritária a todas as instâncias do conjunto de teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2c4d03e6",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0]\n"
     ]
    }
   ],
   "source": [
    "class_counts = db_treino['Renda'].value_counts()\n",
    "majority_class = class_counts.idxmax()\n",
    "majority_class_proportion = class_counts[majority_class] / len(db_treino)\n",
    "\n",
    "def baseline_predict():\n",
    "    return np.repeat(majority_class, len(db_teste))\n",
    "\n",
    "baseline_predictions = baseline(db_treino, db_teste, 'Renda')\n",
    "print(f'{baseline_predictions}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08a7e1c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A classe mais frequente na base de dados é: -10.0 e possui 12735 exemplos\n"
     ]
    }
   ],
   "source": [
    "# Calcula a frequência de cada classe no conjunto de treinamento\n",
    "def baseline(train_data, test_data, target_column):\n",
    "    class_counts = {}\n",
    "    for _, row in train_data.iterrows():\n",
    "        class_label = row[target_column]\n",
    "        if class_label in class_counts:\n",
    "            class_counts[class_label] += 1\n",
    "        else:\n",
    "            class_counts[class_label] = 1\n",
    "\n",
    "    majority_class = max(class_counts, key=class_counts.get)\n",
    "    majority_class_count = class_counts[majority_class]\n",
    "\n",
    "    predictions = [majority_class] * len(test_data)\n",
    "\n",
    "    return majority_class, majority_class_count\n",
    "\n",
    "majority_class, majority_class_count = baseline(db_treino, db_teste, 'Renda')\n",
    "print(f'A classe mais frequente na base de dados é: {majority_class} e possui {majority_class_count} exemplos')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5bc76c",
   "metadata": {},
   "source": [
    "> Como a base de dados foi padronizada com valores entre o intervalo numérico de **[-10 a +10]**, a classe mais frequente na base de dados é **-10**, ou seja, que representa justamente o valor **<=50K** assumido pelo atributo **Renda.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30393fe5",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# K-NN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6942d995",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> O algoritmo **K-NN** atribui rótulos às instâncias de teste com base nos rótulos das instâncias vizinhas mais próximas no espaço de atributos. O valor de **k** determina o **número de vizinhos** considerados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "822069fc",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "def knn_model(train_data, test_data, k):\n",
    "    train_data = train_data.copy()\n",
    "    test_data = test_data.copy()\n",
    "\n",
    "    X_train = train_data.drop('Renda', axis=1)\n",
    "    y_train = train_data['Renda']\n",
    "    X_test = test_data.drop('Renda', axis=1)\n",
    "    y_test = test_data['Renda']\n",
    "\n",
    "    for column in X_train.columns:\n",
    "        if X_train[column].dtype != 'object':\n",
    "            X_train[column] = pd.cut(X_train[column], bins=5, labels=False)\n",
    "            X_test[column] = pd.cut(X_test[column], bins=5, labels=False)\n",
    "\n",
    "    y_train = convert_labels_to_numeric(y_train)\n",
    "    y_test = convert_labels_to_numeric(y_test)\n",
    "\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "\n",
    "    knn.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = knn.predict(X_test)\n",
    "\n",
    "    accuracy = calculate_accuracy(y_test, y_pred)\n",
    "    precision = calculate_precision(y_test, y_pred)\n",
    "    recall = calculate_recall(y_test, y_pred)\n",
    "    confusion = calculate_confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    return accuracy, precision, recall, confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "93f74a0c",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>confusion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.719420</td>\n",
       "      <td>0.801014</td>\n",
       "      <td>0.800760</td>\n",
       "      <td>[[695, 628], [629, 2528]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0.770089</td>\n",
       "      <td>0.806572</td>\n",
       "      <td>0.886284</td>\n",
       "      <td>[[652, 671], [359, 2798]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>0.785268</td>\n",
       "      <td>0.804438</td>\n",
       "      <td>0.918594</td>\n",
       "      <td>[[618, 705], [257, 2900]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50</td>\n",
       "      <td>0.785491</td>\n",
       "      <td>0.796436</td>\n",
       "      <td>0.934431</td>\n",
       "      <td>[[569, 754], [207, 2950]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100</td>\n",
       "      <td>0.786161</td>\n",
       "      <td>0.788356</td>\n",
       "      <td>0.952170</td>\n",
       "      <td>[[516, 807], [151, 3006]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     k  accuracy  precision    recall                  confusion\n",
       "0    1  0.719420   0.801014  0.800760  [[695, 628], [629, 2528]]\n",
       "1    5  0.770089   0.806572  0.886284  [[652, 671], [359, 2798]]\n",
       "2   20  0.785268   0.804438  0.918594  [[618, 705], [257, 2900]]\n",
       "3   50  0.785491   0.796436  0.934431  [[569, 754], [207, 2950]]\n",
       "4  100  0.786161   0.788356  0.952170  [[516, 807], [151, 3006]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "k_values = [1, 5, 20, 50, 100]\n",
    "results = []\n",
    "\n",
    "for k in k_values:\n",
    "    accuracy, precision, recall, confusion = knn_model(db_treino, db_teste, k)\n",
    "    results.append([k, accuracy, precision, recall, confusion])\n",
    "\n",
    "columns = ['k', 'accuracy', 'precision', 'recall', 'confusion']\n",
    "metricas_res = pd.DataFrame(results, columns=columns)\n",
    "display(metricas_res)\n",
    "#display(confusao)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894412f9",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> É possível verificar médidas do desempenho do algoritmo K-NN com diversas variações de **K**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca238ec",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Árvore C4.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a1b193",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> O algoritmo **C4.5** é um algoritmo de árvore de decisão que constrói uma árvore de decisão onde cada nó interno representa um teste em um atributo e cada ramo representa um resultado desse teste. \n",
    "> \n",
    "> O **C4.5** utiliza o ganho de informação para determinar a melhor divisão dos dados em cada nó da árvore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8cd2eff2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "def arvorec4_5(train_data, test_data, criterio):\n",
    "    train_data = train_data.copy()\n",
    "    test_data = test_data.copy()\n",
    "\n",
    "    X_train = train_data.drop('Renda', axis=1)\n",
    "    y_train = train_data['Renda']\n",
    "    X_test = test_data.drop('Renda', axis=1)\n",
    "    y_test = test_data['Renda']\n",
    "\n",
    "    for column in X_train.columns:\n",
    "        if X_train[column].dtype != 'object':\n",
    "            X_train[column] = pd.cut(X_train[column], bins=5, labels=False)\n",
    "            X_test[column] = pd.cut(X_test[column], bins=5, labels=False)\n",
    "\n",
    "    y_train = convert_labels_to_numeric(y_train)\n",
    "    y_test = convert_labels_to_numeric(y_test)\n",
    "\n",
    "    c4_5 = DecisionTreeClassifier(criterion=criterio)\n",
    "    c4_5.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = c4_5.predict(X_test)\n",
    "\n",
    "    accuracy = calculate_accuracy(y_test, y_pred)\n",
    "    precision = calculate_precision(y_test, y_pred)\n",
    "    recall = calculate_recall(y_test, y_pred)\n",
    "    confusion = calculate_confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    return accuracy, precision, recall, confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "969fdfe7",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>criterio</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>confusion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>entropy</td>\n",
       "      <td>0.785937</td>\n",
       "      <td>0.812039</td>\n",
       "      <td>0.905923</td>\n",
       "      <td>[[661, 662], [297, 2860]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gini</td>\n",
       "      <td>0.786384</td>\n",
       "      <td>0.812323</td>\n",
       "      <td>0.906240</td>\n",
       "      <td>[[662, 661], [296, 2861]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>friedman_mse</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.811985</td>\n",
       "      <td>0.905607</td>\n",
       "      <td>[[661, 662], [298, 2859]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mse</td>\n",
       "      <td>0.786607</td>\n",
       "      <td>0.812376</td>\n",
       "      <td>0.906557</td>\n",
       "      <td>[[662, 661], [295, 2862]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       criterio  accuracy  precision    recall                  confusion\n",
       "0       entropy  0.785937   0.812039  0.905923  [[661, 662], [297, 2860]]\n",
       "1          gini  0.786384   0.812323  0.906240  [[662, 661], [296, 2861]]\n",
       "2  friedman_mse  0.785714   0.811985  0.905607  [[661, 662], [298, 2859]]\n",
       "3           mse  0.786607   0.812376  0.906557  [[662, 661], [295, 2862]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = []\n",
    "metricas = ['entropy', 'gini', 'friedman_mse', 'mse']\n",
    "\n",
    "for metrica in metricas:\n",
    "    accuracy, precision, recall, confusion = arvorec4_5(db_treino, db_teste, 'entropy')\n",
    "    results.append([metrica, accuracy, precision, recall, confusion])\n",
    "    \n",
    "columns = ['criterio', 'accuracy', 'precision', 'recall', 'confusion']\n",
    "\n",
    "metricas_res = pd.DataFrame(results, columns=columns)\n",
    "display(metricas_res)\n",
    "#display(confusao)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c053b3d",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Possível verificar o desempenho do modelo com base em diferentes métricas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33941d57",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd097f4b",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> **MLP**  é uma arquitetura de rede neural artificial composta por múltiplas camadas de neurônios. Cada neurônio em uma camada está conectado a todos os neurônios nas camadas adjacentes. \n",
    "> \n",
    "> O **MLP** é treinado usando o algoritmo de backpropagation, que ajusta os pesos das conexões para minimizar a diferença entre as saídas previstas e os valores de destino."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ba0634",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> retirar bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bb9fa4d7",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>activation</th>\n",
       "      <th>solver</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>confusion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(50,)</td>\n",
       "      <td>relu</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.787723</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.787723</td>\n",
       "      <td>[[2879, 285], [666, 650]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(50,)</td>\n",
       "      <td>relu</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.781920</td>\n",
       "      <td>0.771210</td>\n",
       "      <td>0.781920</td>\n",
       "      <td>[[2902, 262], [715, 601]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(50,)</td>\n",
       "      <td>logistic</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.795089</td>\n",
       "      <td>0.788400</td>\n",
       "      <td>0.795089</td>\n",
       "      <td>[[2960, 204], [714, 602]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(50,)</td>\n",
       "      <td>logistic</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.779893</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>[[2985, 179], [781, 535]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(50,)</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.791741</td>\n",
       "      <td>0.784428</td>\n",
       "      <td>0.791741</td>\n",
       "      <td>[[2956, 208], [725, 591]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(50,)</td>\n",
       "      <td>tanh</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.786607</td>\n",
       "      <td>0.778177</td>\n",
       "      <td>0.786607</td>\n",
       "      <td>[[2948, 216], [740, 576]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(25,)</td>\n",
       "      <td>relu</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.787500</td>\n",
       "      <td>0.782491</td>\n",
       "      <td>0.787500</td>\n",
       "      <td>[[2991, 173], [779, 537]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(25,)</td>\n",
       "      <td>relu</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.787277</td>\n",
       "      <td>0.779998</td>\n",
       "      <td>0.787277</td>\n",
       "      <td>[[2965, 199], [754, 562]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(25,)</td>\n",
       "      <td>logistic</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.790848</td>\n",
       "      <td>0.784083</td>\n",
       "      <td>0.790848</td>\n",
       "      <td>[[2966, 198], [739, 577]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(25,)</td>\n",
       "      <td>logistic</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.779170</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>[[2977, 187], [773, 543]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>(25,)</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.796205</td>\n",
       "      <td>0.789696</td>\n",
       "      <td>0.796205</td>\n",
       "      <td>[[2961, 203], [710, 606]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>(25,)</td>\n",
       "      <td>tanh</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.787500</td>\n",
       "      <td>0.779302</td>\n",
       "      <td>0.787500</td>\n",
       "      <td>[[2950, 214], [738, 578]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>(100,)</td>\n",
       "      <td>relu</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.786384</td>\n",
       "      <td>0.776241</td>\n",
       "      <td>0.786384</td>\n",
       "      <td>[[2887, 277], [680, 636]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>(100,)</td>\n",
       "      <td>relu</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.786161</td>\n",
       "      <td>0.775998</td>\n",
       "      <td>0.786161</td>\n",
       "      <td>[[2890, 274], [684, 632]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>(100,)</td>\n",
       "      <td>logistic</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.793527</td>\n",
       "      <td>0.786193</td>\n",
       "      <td>0.793527</td>\n",
       "      <td>[[2952, 212], [713, 603]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>(100,)</td>\n",
       "      <td>logistic</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.785045</td>\n",
       "      <td>0.778242</td>\n",
       "      <td>0.785045</td>\n",
       "      <td>[[2975, 189], [774, 542]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>(100,)</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.790179</td>\n",
       "      <td>0.780646</td>\n",
       "      <td>0.790179</td>\n",
       "      <td>[[2894, 270], [670, 646]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>(100,)</td>\n",
       "      <td>tanh</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.785491</td>\n",
       "      <td>0.777128</td>\n",
       "      <td>0.785491</td>\n",
       "      <td>[[2952, 212], [749, 567]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   hidden_layers activation solver  accuracy  precision    recall  \\\n",
       "0          (50,)       relu   adam  0.787723   0.777778  0.787723   \n",
       "1          (50,)       relu    sgd  0.781920   0.771210  0.781920   \n",
       "2          (50,)   logistic   adam  0.795089   0.788400  0.795089   \n",
       "3          (50,)   logistic    sgd  0.785714   0.779893  0.785714   \n",
       "4          (50,)       tanh   adam  0.791741   0.784428  0.791741   \n",
       "5          (50,)       tanh    sgd  0.786607   0.778177  0.786607   \n",
       "6          (25,)       relu   adam  0.787500   0.782491  0.787500   \n",
       "7          (25,)       relu    sgd  0.787277   0.779998  0.787277   \n",
       "8          (25,)   logistic   adam  0.790848   0.784083  0.790848   \n",
       "9          (25,)   logistic    sgd  0.785714   0.779170  0.785714   \n",
       "10         (25,)       tanh   adam  0.796205   0.789696  0.796205   \n",
       "11         (25,)       tanh    sgd  0.787500   0.779302  0.787500   \n",
       "12        (100,)       relu   adam  0.786384   0.776241  0.786384   \n",
       "13        (100,)       relu    sgd  0.786161   0.775998  0.786161   \n",
       "14        (100,)   logistic   adam  0.793527   0.786193  0.793527   \n",
       "15        (100,)   logistic    sgd  0.785045   0.778242  0.785045   \n",
       "16        (100,)       tanh   adam  0.790179   0.780646  0.790179   \n",
       "17        (100,)       tanh    sgd  0.785491   0.777128  0.785491   \n",
       "\n",
       "                    confusion  \n",
       "0   [[2879, 285], [666, 650]]  \n",
       "1   [[2902, 262], [715, 601]]  \n",
       "2   [[2960, 204], [714, 602]]  \n",
       "3   [[2985, 179], [781, 535]]  \n",
       "4   [[2956, 208], [725, 591]]  \n",
       "5   [[2948, 216], [740, 576]]  \n",
       "6   [[2991, 173], [779, 537]]  \n",
       "7   [[2965, 199], [754, 562]]  \n",
       "8   [[2966, 198], [739, 577]]  \n",
       "9   [[2977, 187], [773, 543]]  \n",
       "10  [[2961, 203], [710, 606]]  \n",
       "11  [[2950, 214], [738, 578]]  \n",
       "12  [[2887, 277], [680, 636]]  \n",
       "13  [[2890, 274], [684, 632]]  \n",
       "14  [[2952, 212], [713, 603]]  \n",
       "15  [[2975, 189], [774, 542]]  \n",
       "16  [[2894, 270], [670, 646]]  \n",
       "17  [[2952, 212], [749, 567]]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "def mlp_model(train_data, test_data, hidden_layer_sizes=(50,), activation='relu', solver='adam', random_state=42):\n",
    "    train_data = train_data.copy()\n",
    "    test_data = test_data.copy()\n",
    "\n",
    "    X_train = train_data.drop('Renda', axis=1)\n",
    "    y_train = train_data['Renda']\n",
    "    X_test = test_data.drop('Renda', axis=1)\n",
    "    y_test = test_data['Renda']\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "        \n",
    "        mlp = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes, activation=activation, solver=solver, max_iter=200, random_state=random_state)\n",
    "        mlp.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = mlp.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted', zero_division=1)\n",
    "    recall = recall_score(y_test, y_pred, average='weighted', zero_division=1)\n",
    "    confusion = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    return accuracy, precision, recall, confusion\n",
    "\n",
    "results = []\n",
    "hidden_layer_sizes = [(50,), (25,), (100,)]\n",
    "activations = ['relu', 'logistic', 'tanh']\n",
    "solvers = ['adam', 'sgd']\n",
    "random_state = 42\n",
    "\n",
    "for hidden_layers in hidden_layer_sizes:\n",
    "    for activation in activations:\n",
    "        for solver in solvers:\n",
    "            accuracy, precision, recall, confusion = mlp_model(db_treino, db_teste, hidden_layer_sizes=hidden_layers, activation=activation, solver=solver, random_state=random_state)\n",
    "            results.append([hidden_layers, activation, solver, accuracy, precision, recall, confusion])\n",
    "\n",
    "columns = ['hidden_layers', 'activation', 'solver', 'accuracy', 'precision', 'recall', 'confusion']\n",
    "metricas_res = pd.DataFrame(results, columns=columns)\n",
    "display(metricas_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de078a17",
   "metadata": {},
   "source": [
    "# Resultados Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3c07ddae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia: 0.7118303571428571\n",
      "Precisão: 0.7118303571428571\n",
      "Recall: 1.0\n",
      "Matriz de Confusão:\n",
      "Predito  -10.0\n",
      "Real          \n",
      "-10.0     3189\n",
      " 10.0     1291\n"
     ]
    }
   ],
   "source": [
    "predictions = baseline_predict()\n",
    "\n",
    "accuracy = (predictions == db_teste['Renda']).mean()\n",
    "\n",
    "confusion_matrix = pd.crosstab(db_teste['Renda'], predictions, rownames=['Real'], colnames=['Predito'], dropna=False)\n",
    "\n",
    "true_positive = confusion_matrix[majority_class][majority_class]\n",
    "false_positive = confusion_matrix[majority_class].sum() - true_positive\n",
    "false_negative = confusion_matrix.drop(majority_class, axis=1).sum().sum()\n",
    "\n",
    "precision = true_positive / (true_positive + false_positive)\n",
    "recall = true_positive / (true_positive + false_negative)\n",
    "\n",
    "# Imprimir os resultados\n",
    "print(\"Acurácia:\", accuracy)\n",
    "print(\"Precisão:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"Matriz de Confusão:\")\n",
    "print(confusion_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ad28cecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy_baseline(y_true, majority_class):\n",
    "    correct_predictions = y_true.count(majority_class)\n",
    "    total_predictions = len(y_true)\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    return accuracy\n",
    "\n",
    "def calculate_precision_baseline(y_true, majority_class):\n",
    "    true_positives = y_true.count(majority_class)\n",
    "    total_predictions = len(y_true)\n",
    "    false_positives = total_predictions - true_positives\n",
    "    if true_positives + false_positives == 0:\n",
    "        precision = 0\n",
    "    else:\n",
    "        precision = true_positives / (true_positives + false_positives)\n",
    "    return precision\n",
    "\n",
    "def calculate_recall_baseline(y_true, majority_class):\n",
    "    true_positives = y_true.count(majority_class)\n",
    "    false_negatives = len(y_true) - true_positives\n",
    "    if true_positives + false_negatives == 0:\n",
    "        recall = 0\n",
    "    else:\n",
    "        recall = true_positives / (true_positives + false_negatives)\n",
    "    return recall\n",
    "\n",
    "def calculate_confusion_matrix_baseline(y_true, majority_class):\n",
    "    true_positives = y_true.count(majority_class)\n",
    "    true_negatives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = len(y_true) - true_positives\n",
    "    confusion_matrix = [[true_negatives, false_positives], [false_negatives, true_positives]]\n",
    "    return confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2acf390a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics_baseline():\n",
    "    accuracy = calculate_accuracy_baseline(y_true, baseline_predictions)\n",
    "    precision = calculate_precision_baseline(y_true, baseline_predictions)\n",
    "    recall = calculate_recall_baseline(y_true, baseline_predictions)\n",
    "    confusion = calculate_confusion_matrix_baseline(y_true, baseline_predictions)\n",
    "    return accuracy, precision, recall, confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b878b7f5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def baseline(train_data, test_data, target_column):\n",
    "    class_counts = train_data[target_column].value_counts()\n",
    "    majority_class = class_counts.idxmax()\n",
    "    majority_class_count = class_counts.max()\n",
    "    predictions = [majority_class] * len(test_data)\n",
    "    return predictions, majority_class, majority_class_count\n",
    "\n",
    "#print(f'Majority Class: {majority_class}')\n",
    "#print(f'Majority Class Count: {majority_class_count}')\n",
    "#print(f'y_true: {y_true}')\n",
    "#print(f'Baseline Predictions: {baseline_predictions}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0f3b699d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia: 0.0\n",
      "Precisão: 0.0\n",
      "Recall: 0.0\n",
      "Matriz de Confusão: [[0, 0], [4480, 0]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>confusion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[0, 0], [4480, 0]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  precision  recall            confusion\n",
       "0       0.0        0.0     0.0  [[0, 0], [4480, 0]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "baseline_predictions, majority_class, majority_class_count = baseline(db_treino, db_teste, 'Renda')\n",
    "y_true = db_teste['Renda'].tolist()\n",
    "\n",
    "accuracy, precision, recall, confusion = calculate_metrics_baseline()\n",
    "\n",
    "print(f'Acurácia: {accuracy}')\n",
    "print(f'Precisão: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'Matriz de Confusão: {confusion}')\n",
    "\n",
    "results=[]\n",
    "results.append([accuracy, precision, recall, confusion])\n",
    "columns = ['accuracy', 'precision', 'recall', 'confusion']\n",
    "metricas_res = pd.DataFrame(results, columns=columns)\n",
    "display(metricas_res)\n",
    "#display(confusao)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e8a2ed",
   "metadata": {},
   "source": [
    "# Resultados 3 algoritmos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b528e35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
